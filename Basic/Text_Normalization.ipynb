{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Normalization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSvk9gGvMWQg",
        "outputId": "e40dc49c-8fff-4430-e65a-2349c91594d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = 'The Matrix is everywhre its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "sentence = sent_tokenize(text_sample)"
      ],
      "metadata": {
        "id": "ki1quBIdMlg5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(sentence), len(sentence))\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKHkQec3NG3I",
        "outputId": "bc183c1e-a7a8-4ddb-9adf-2cbf95fddfbe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhre its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'The Matrix is everywhre its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I5dbsnmNLee",
        "outputId": "c89c42f8-6f5e-4373-8fc4-db66a05aa1e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhre', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "# 여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수를 정의\n",
        "def tokenize_text(text):\n",
        "  # 문장별로 분리\n",
        "  sentences = sent_tokenize(text)\n",
        "  # 분리된 문장별 단어 토큰화\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens\n",
        "\n",
        "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt6AAEqZN92g",
        "outputId": "2b764ad0-cce3-4201-ee92-1dcafc7105fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhre', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장을 단어별로 하나씩 토큰화 하는 경우 문맥적인 의미는 무시될 수 밖에 없다.  \n",
        "이러한 문제를 해결하기 위해 도입 된 것이 `n-gram`이다."
      ],
      "metadata": {
        "id": "OSAmsTijO5Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 스톱워드 확인\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print('영어 stop word 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7buCI7OOy1o",
        "outputId": "f7d34a8c-31b1-4861-ea7d-fca1866e0617"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "영어 stop word 갯수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스톱워드 제거\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens:\n",
        "  filtered_word = []\n",
        "  # 개별 문장별로 토큰회된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
        "  for word in sentence:\n",
        "    # 소문자로 모두 반환\n",
        "    word = word.lower()\n",
        "    # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "    if word not in stopwords:\n",
        "      filtered_word.append(word)\n",
        "    all_tokens.append(filtered_word)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQaKXyxCPysJ",
        "outputId": "3d33eb13-6ade-459b-957c-1b2191d4c191"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['matrix', 'everywhre', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemmer를 사용한 어근 추출\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('work'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8gNwDbWQnbo",
        "outputId": "0253b68b-d7cb-4d05-d0f7-7c2027adac27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`LancasterStemmer`** 의 경우 형용사의 비교급 최상급으로 변형된 단어의 경우 정확한 원형을 찾지 못한다."
      ],
      "metadata": {
        "id": "SuitKV3jRsPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeYswcJSRmbQ",
        "outputId": "eb4b43f2-1223-4ce8-fe93-467df56b7a1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizer를 이용한 어근 추출\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
        "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Imh1L_zS1iS",
        "outputId": "c11f9f23-6790-4579-a641-681b247dcc64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ]
    }
  ]
}